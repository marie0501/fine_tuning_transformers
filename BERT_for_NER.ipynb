{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2Sl95A9YEwjSiZ5rgfDRY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marie0501/fine_tuning_transformers/blob/main/BERT_for_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch dev https://github.com/rolysr/medical-knowledge-discoverer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8hfLhhQCPge",
        "outputId": "7e249774-23c4-4ee3-b9d3-5cf776aeb566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'medical-knowledge-discoverer'...\n",
            "remote: Enumerating objects: 550, done.\u001b[K\n",
            "remote: Counting objects: 100% (211/211), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 550 (delta 96), reused 157 (delta 60), pack-reused 339\u001b[K\n",
            "Receiving objects: 100% (550/550), 2.37 MiB | 5.69 MiB/s, done.\n",
            "Resolving deltas: 100% (243/243), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd medical-knowledge-discoverer/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUtnDRCPCV7G",
        "outputId": "cb19da9c-808e-48c3-d1d1-741bdca577ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/medical-knowledge-discoverer\n",
            "datasets  main.py  ontology   requirements.txt\n",
            "LICENSE   models   README.md  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCXZgu3XCk0v",
        "outputId": "cfb49a0b-5261-450d-d22b-97f1bc00b635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es_core_news_sm==3.1.0 (from -r requirements.txt (line 1))\n",
            "  Downloading es_core_news_sm-3.1.0-py3-none-any.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext==0.9.2 (from -r requirements.txt (line 2))\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras==2.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.12.0)\n",
            "Collecting matplotlib==3.5.2 (from -r requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx==2.8.4 (from -r requirements.txt (line 5))\n",
            "  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.21.5 (from -r requirements.txt (line 6))\n",
            "  Downloading numpy-1.21.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.4.4 (from -r requirements.txt (line 7))\n",
            "  Downloading pandas-1.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py2neo==2021.2.3 (from -r requirements.txt (line 8))\n",
            "  Downloading py2neo-2021.2.3-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.0/177.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit_learn==1.0.2 (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==3.5.3 (from -r requirements.txt (line 10))\n",
            "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.12.0)\n",
            "INFO: pip is looking at multiple versions of es-core-news-sm to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install -r requirements.txt (line 1) and spacy==3.5.3 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested spacy==3.5.3\n",
            "    es-core-news-sm 3.1.0 depends on spacy<3.2.0 and >=3.1.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install transformers"
      ],
      "metadata": {
        "id": "34lWv7BS3_TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pathlib import Path\n",
        "from utils.anntools import Collection\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import torch \n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.optim import SGD"
      ],
      "metadata": {
        "id": "1x2xoWGjB2Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leer Data y crear Dataframe"
      ],
      "metadata": {
        "id": "G2UiZ1qhJ4kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_collection = Collection()\n",
        "for fname in Path(\"datasets/train\").rglob(\"*.txt\"):\n",
        "    train_collection.load(fname)\n",
        "\n",
        "test_collection = Collection()\n",
        "for fname in Path(\"datasets/test/scenario2-taskA\").rglob(\"*.txt\"):\n",
        "    test_collection.load(fname)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_dataframe(collection):\n",
        "    lb = []\n",
        "    txt = []\n",
        "    for sentence in collection.sentences:\n",
        "        new_sentence = sentence.text.replace('(','').replace(')','')\n",
        "        txt.append(new_sentence)\n",
        "        labels = ['O' for i in range(len(new_sentence.split(' ')))]\n",
        "        keyphrases = sentence.keyphrases\n",
        "        sentence_list = new_sentence.split(' ')\n",
        "        count = 0\n",
        "        k = 0\n",
        "        for keyphrase in sentence.keyphrases:\n",
        "            index = new_sentence.find(keyphrase.text)   \n",
        "            while count + k < index:\n",
        "                count += len(sentence_list[k])\n",
        "                k+=1\n",
        "            keyphrases_list = keyphrase.text.split(' ')\n",
        "            if k < len(sentence_list) and sentence_list[k] == keyphrases_list[0]:\n",
        "                labels[k]= 'B-' + keyphrase.label \n",
        "                count += len(keyphrases_list[0])\n",
        "                i = 1\n",
        "                while i < len(keyphrases_list):\n",
        "                    labels[k + i] = 'I-' + keyphrase.label \n",
        "                    count += len(keyphrases_list[i])\n",
        "                    i += 1\n",
        "                k +=i\n",
        "        lb.append(labels.copy())\n",
        "\n",
        "    labels = []\n",
        "\n",
        "    for label_list in lb:\n",
        "      current_label = \"\"\n",
        "      for label in label_list:\n",
        "        current_label += label + ' '\n",
        "      labels.append(current_label)\n",
        "\n",
        "    data = {\n",
        "        \"text\":txt,\n",
        "        \"labels\":labels\n",
        "    }\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    return df\n",
        "\n",
        "df = build_dataframe(train_collection)\n",
        "df_test = build_dataframe(test_collection) # esto hay que arreglarlo para que tenga los labels\n",
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ey2BQWaxDdDQ",
        "outputId": "37834f11-7233-4c01-941a-f535cd6b1157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  Los glóbulos blancos ayudan a su organismo a c...   \n",
              "1  Se contagia a través de relaciones sexuales co...   \n",
              "2  Las lágrimas son producidas por las glándulas ...   \n",
              "3  Con frecuencia, las adolescentes no reciben cu...   \n",
              "4  Phage-mediated horizontal gene transfer events...   \n",
              "\n",
              "                                              labels  \n",
              "0                               O O O O O O O O O O   \n",
              "1                             O O O O O O O O O O O   \n",
              "2               O O O O O O O O O O O O O O O O O O   \n",
              "3                   O O O O O O O O O O O O O O O O   \n",
              "4  O O O O O O O O O O O O O O O O O O O O O O O ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3f853cf-ab9c-4ae2-a357-38f60ad9d2cb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Los glóbulos blancos ayudan a su organismo a c...</td>\n",
              "      <td>O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Se contagia a través de relaciones sexuales co...</td>\n",
              "      <td>O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Las lágrimas son producidas por las glándulas ...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Con frecuencia, las adolescentes no reciben cu...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Phage-mediated horizontal gene transfer events...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3f853cf-ab9c-4ae2-a357-38f60ad9d2cb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d3f853cf-ab9c-4ae2-a357-38f60ad9d2cb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d3f853cf-ab9c-4ae2-a357-38f60ad9d2cb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicializar Tokenizer\n"
      ],
      "metadata": {
        "id": "_fvBql04D8Mp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjAl0xVh3tgP"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear clase Dataset"
      ],
      "metadata": {
        "id": "hne5MV6GEyNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac7f0682-ea50-4aeb-bcd3-9230df735554"
      },
      "outputs": [],
      "source": [
        "label_all_tokens = False\n",
        "\n",
        "def align_label(texts, labels):\n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "class DataSequence(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
        "        print(f\"lb {lb}\")\n",
        "        txt = df['text'].values.tolist()\n",
        "        print(f\"txt {txt}\")\n",
        "        self.texts = [tokenizer(str(i),\n",
        "                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n",
        "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
        "\n",
        "        print(f\" texts {self.texts[0]}\")\n",
        "        print(f\"labels {self.labels[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_data(self, idx):\n",
        "\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "\n",
        "        return torch.LongTensor(self.labels[idx])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_data = self.get_batch_data(idx)\n",
        "        batch_labels = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_data, batch_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separar data y definir etiquetas únicas "
      ],
      "metadata": {
        "id": "-J8Fmk10eVIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = {'B-Action', 'B-Concept', 'B-Predicate', 'B-Reference', 'I-Action', 'I-Concept', 'I-Predicate',  'B-Reference', 'O'}\n",
        "\n",
        "labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n",
        "ids_to_labels = {v: k for v, k in enumerate(unique_labels)}\n",
        "\n",
        "df_train, df_val = np.split(df.sample(frac=1, random_state=42),\n",
        "                            [int(.8 * len(df))])"
      ],
      "metadata": {
        "id": "p_tI1vNreePd",
        "outputId": "4b2fe614-374f-47a4-e1d6-154191c42ec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'B-Action',\n",
              " 1: 'B-Concept',\n",
              " 2: 'I-Predicate',\n",
              " 3: 'B-Predicate',\n",
              " 4: 'I-Action',\n",
              " 5: 'B-Reference',\n",
              " 6: 'O',\n",
              " 7: 'I-Concept'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construir modelo"
      ],
      "metadata": {
        "id": "vX4kqVRJghw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(BertModel, self).__init__()\n",
        "\n",
        "        self.bert = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(unique_labels))\n",
        "\n",
        "    def forward(self, input_id, mask, label):\n",
        "\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0dET3Zipgm_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenando Modelo"
      ],
      "metadata": {
        "id": "qmNCWympgqmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, df_train, df_val):\n",
        "\n",
        "    train_dataset = DataSequence(df_train)\n",
        "    val_dataset = DataSequence(df_val)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    best_acc = 0\n",
        "    best_loss = 1000\n",
        "\n",
        "    for epoch_num in range(EPOCHS):\n",
        "\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for train_data, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, logits = model(input_id, mask, train_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][train_label[i] != -100]\n",
        "              label_clean = train_label[i][train_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_train += acc\n",
        "              total_loss_train += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        for val_data, val_label in val_dataloader:\n",
        "\n",
        "            val_label = val_label.to(device)\n",
        "            mask = val_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = val_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            loss, logits = model(input_id, mask, val_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][val_label[i] != -100]\n",
        "              label_clean = val_label[i][val_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_val += acc\n",
        "              total_loss_val += loss.item()\n",
        "\n",
        "        val_accuracy = total_acc_val / len(df_val)\n",
        "        val_loss = total_loss_val / len(df_val)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
        "\n",
        "LEARNING_RATE = 5e-3\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "model = BertModel()\n",
        "train_loop(model, df_train, df_val)"
      ],
      "metadata": {
        "id": "dweOFpk7gtnM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}